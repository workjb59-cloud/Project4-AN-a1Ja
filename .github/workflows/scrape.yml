name: Scrape Aljarida Archive

on:
  # Run manually from Actions tab
  workflow_dispatch:
    inputs:
      start_date:
        description: 'Start date (YYYY-MM-DD)'
        required: false
        default: '2007-06-02'
      end_date:
        description: 'End date (YYYY-MM-DD)'
        required: false
        default: ''
  
  # Schedule to run daily at 2 AM UTC (optional - comment out if not needed)
 # schedule:
  #  - cron: '0 2 * * *'

jobs:
  scrape:
    runs-on: ubuntu-latest
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
      
      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'
      
      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt
      
      - name: Run scraper
        env:
          AWS_ACCESS_KEY_ID: ${{ secrets.AWS_ACCESS_KEY_ID }}
          AWS_SECRET_ACCESS_KEY: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          S3_BUCKET_NAME: ${{ secrets.S3_BUCKET_NAME }}
        run: |
          if [ -n "${{ github.event.inputs.start_date }}" ] && [ -n "${{ github.event.inputs.end_date }}" ]; then
            python scraper.py "${{ github.event.inputs.start_date }}" "${{ github.event.inputs.end_date }}"
          elif [ -n "${{ github.event.inputs.start_date }}" ]; then
            python scraper.py "${{ github.event.inputs.start_date }}"
          else
            python scraper.py
          fi
      
      - name: Upload logs as artifact (on failure)
        if: failure()
        uses: actions/upload-artifact@v4
        with:
          name: scraper-logs
          path: |
            *.log
            **/*.log
          retention-days: 7
